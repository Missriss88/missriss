import streamlit as st
from ultralytics import YOLO
import cv2
import numpy as np
import google.generativeai as genai
import av
from streamlit_webrtc import webrtc_streamer
import tempfile
import os
import requests
import yt_dlp
from collections import Counter
from datetime import datetime

# --- ì„¤ì • ---
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except (KeyError, FileNotFoundError):
    st.error("ì˜¤ë¥˜: .streamlit/secrets.toml íŒŒì¼ì— 'GOOGLE_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def load_yolo_model(path):
    try:
        model = YOLO(path)
        return model
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None

model = load_yolo_model('fire2.pt')
if model is None:
    st.stop()

# --- Gemini LLM ì—°ë™ í•¨ìˆ˜ ---
def get_llm_response(detection_summary):
    genai.configure(api_key=GOOGLE_API_KEY)
    if not detection_summary: return "íƒì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤."
    prompt = (
        f"í™”ì¬ ê°ì§€ ì‹œìŠ¤í…œì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ê°ì²´ë“¤ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤: '{detection_summary}'.\n"
        "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:\n"
        "1. ì¦‰ì‹œ í•„ìš”í•œ ì¡°ì¹˜ (ì˜ˆ: 'ì†Œí™”ê¸° í™•ì¸', 'ì¦‰ì‹œ ëŒ€í”¼', '119 ì‹ ê³ ')\n"
        "2. ì‚¬ì§„ì†ì˜ ìƒí™©ì„ ì •í™•í•˜ê²Œ íŒë‹¨í•˜ì—¬ ë‹µì„ ì¤„ê²ƒ(ì´ë¯¸ ì§„í™”ë˜ì—ˆëŠ”ì§€, ì•„ì§ í™”ì¬ì¤‘ì¸ì§€)\n"
        "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, ê° í•­ëª©ì„ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.  "
    )
    try:
        gemini_model = genai.GenerativeModel('gemini-2.5-flash')
        response = gemini_model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# --- ì»¤ìŠ¤í…€ ë“œë¡œì‰ í•¨ìˆ˜ ---
CLASS_COLORS = {"fire": (0, 0, 255), "smoke": (128, 128, 128)}
def draw_detections(image, results):
    img_copy = image.copy()
    if results and results[0].boxes is not None:
        boxes = results[0].boxes.xyxy.cpu().numpy()
        classes = results[0].boxes.cls.cpu().numpy().astype(int)
        confidences = results[0].boxes.conf.cpu().numpy()
        class_names = results[0].names
        for box, cls_id, conf in zip(boxes, classes, confidences):
            x1, y1, x2, y2 = map(int, box)
            class_name = class_names[cls_id]
            color = CLASS_COLORS.get(class_name, (255, 255, 255))
            cv2.rectangle(img_copy, (x1, y1), (x2, y2), color, 2)
            label = f"{class_name} {conf:.2f}"
            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
            cv2.rectangle(img_copy, (x1, y1 - h - 10), (x1 + w, y1 - 5), color, -1)
            cv2.putText(img_copy, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
    return img_copy

# --- ìŠ¤íŠ¸ë¦¼ë¦¿ UI ë° ì„¸ì…˜ ê´€ë¦¬ ---
st.title("ğŸ”¥ ì‹¤ì‹œê°„ í™”ì¬ íƒì§€ ì‹œìŠ¤í…œ (with Gemini AI)")
st.write("---")
st.sidebar.header("âš™ï¸ ì…ë ¥ ì†ŒìŠ¤ ì„ íƒ")
source_option = st.sidebar.selectbox("ë‹¤ìŒ ì¤‘ì—ì„œ ì…ë ¥ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”.", ("ì´ë¯¸ì§€", "ë™ì˜ìƒ", "ì‹¤ì‹œê°„ ì›¹ìº "))

if 'detections' not in st.session_state: st.session_state.detections = []
if 'video_processed' not in st.session_state: st.session_state.video_processed = False
if 'source_option' not in st.session_state: st.session_state.source_option = source_option
if st.session_state.source_option != source_option:
    st.session_state.video_processed = False
    st.session_state.detections = []
    st.session_state.source_option = source_option

# --- ë™ì˜ìƒ/URL ê³µí†µ ì²˜ë¦¬ í•¨ìˆ˜ ---
def process_video_stream(cap):
    st.session_state.detections = []
    stframe = st.empty()
    with st.spinner("ì˜ìƒì„ ì²˜ë¦¬í•˜ë©° ì‹¤ì‹œê°„ íƒì§€ ì¤‘ì…ë‹ˆë‹¤..."):
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            results = model(frame)
            annotated_frame = draw_detections(frame, results)
            stframe.image(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB), channels="RGB")
            if results[0].boxes is not None:
                st.session_state.detections.extend([model.names[int(cls)] for cls in results[0].boxes.cls])
    cap.release()
    st.success("ì˜ìƒ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    st.session_state.video_processed = True

# --- ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ëŠ¥ ---
if source_option == "ì´ë¯¸ì§€":
    uploaded_file = st.sidebar.file_uploader("ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["jpg", "jpeg", "png"])
    if uploaded_file is not None:
        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
        image = cv2.imdecode(file_bytes, 1)
        results = model(image)
        annotated_image = draw_detections(image, results)
        st.image(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB), caption="íƒì§€ ê²°ê³¼ ì´ë¯¸ì§€")
        detections = [model.names[int(cls)] for cls in results[0].boxes.cls] if results[0].boxes is not None else []
        if detections:
            detection_counts = Counter(detections)
            detection_summary = ", ".join([f"{obj} {count}ê°œ" for obj, count in detection_counts.items()])
            st.info(f"íƒì§€ ìš”ì•½: **{detection_summary}**")
            with st.spinner("Gemini AIê°€ ìƒí™©ì„ ìƒì„¸ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                description = get_llm_response(detection_summary)
                st.subheader("ğŸ¤– AI ìƒì„¸ ë¶„ì„")
                st.markdown(description)

# --- ë™ì˜ìƒ ì²˜ë¦¬ ê¸°ëŠ¥ ---
elif source_option == "ë™ì˜ìƒ":
    uploaded_video = st.sidebar.file_uploader("ë™ì˜ìƒ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["mp4", "mov", "avi"])
    if uploaded_video is not None:
        st.video(uploaded_video)
        if st.button("ì—…ë¡œë“œëœ ë™ì˜ìƒ ë¶„ì„ ì‹œì‘"):
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tfile:
                tfile.write(uploaded_video.read())
                video_path = tfile.name
            if video_path:
                process_video_stream(cv2.VideoCapture(video_path))
                os.remove(video_path)
    
    # â­ï¸ ë¦¬í¬íŠ¸ ìƒì„± ë²„íŠ¼ì„ 'ë™ì˜ìƒ' ë¸”ë¡ ì•ˆìœ¼ë¡œ ì´ë™
    if st.session_state.video_processed:
        if st.button("ìµœì¢… ìƒí™© ì •ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±"):
            if st.session_state.detections:
                detection_counts = Counter(st.session_state.detections)
                detection_summary = ", ".join([f"{obj} {count}ê°œ" for obj, count in detection_counts.items()])
                st.info(f"ìµœì¢… íƒì§€ ìš”ì•½: **{detection_summary}**")
                with st.spinner("Gemini AIê°€ ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    description = get_llm_response(detection_summary)
                    st.subheader("ğŸ¤– AI ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸")
                    st.markdown(description)
            else:
                st.info("ì˜ìƒ ì „ì²´ì—ì„œ ê°ì²´ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# --- URL ì²˜ë¦¬ ê¸°ëŠ¥ ---
elif source_option == "URL":
    url = st.sidebar.text_input("ë¶„ì„í•  ë™ì˜ìƒ ì£¼ì†Œ(URL)ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    if url and st.button("URL ë™ì˜ìƒ ë¶„ì„ ì‹œì‘"):
        try:
            ydl_opts = {'format': 'best[ext=mp4]/best', 'noplaylist': True}
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
                video_url = info['url']
            st.video(video_url)
            process_video_stream(cv2.VideoCapture(video_url))
        except Exception as e:
            st.error(f"URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # â­ï¸ ë¦¬í¬íŠ¸ ìƒì„± ë²„íŠ¼ì„ 'URL' ë¸”ë¡ ì•ˆìœ¼ë¡œ ì´ë™
    if st.session_state.video_processed:
        if st.button("ìµœì¢… ìƒí™© ì •ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±"):
            if st.session_state.detections:
                detection_counts = Counter(st.session_state.detections)
                detection_summary = ", ".join([f"{obj} {count}ê°œ" for obj, count in detection_counts.items()])
                st.info(f"ìµœì¢… íƒì§€ ìš”ì•½: **{detection_summary}**")
                with st.spinner("Gemini AIê°€ ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    description = get_llm_response(detection_summary)
                    st.subheader("ğŸ¤– AI ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸")
                    st.markdown(description)
            else:
                st.info("ì˜ìƒ ì „ì²´ì—ì„œ ê°ì²´ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# --- ì‹¤ì‹œê°„ ì›¹ìº  ì²˜ë¦¬ ê¸°ëŠ¥ ---
elif source_option == "ì‹¤ì‹œê°„ ì›¹ìº ":
    st.subheader("ğŸ“¹ ì‹¤ì‹œê°„ ì›¹ìº  íƒì§€")
    st.write("ì•„ë˜ 'START' ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì›¹ìº ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")
    def video_frame_callback(frame):
        img = frame.to_ndarray(format="bgr24")
        results = model(img)
        annotated_frame = draw_detections(img, results)
        detections = [model.names[int(cls)] for cls in results[0].boxes.cls] if results[0].boxes is not None else []
        detection_counts = Counter(detections)
        fire_count = detection_counts.get("fire", 0)
        smoke_count = detection_counts.get("smoke", 0)
        status_text_fire = f"Fire: {fire_count}"
        status_text_smoke = f"Smoke: {smoke_count}"
        cv2.rectangle(annotated_frame, (5, 5), (150, 65), (0, 0, 0), -1)
        cv2.putText(annotated_frame, status_text_fire, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        cv2.putText(annotated_frame, status_text_smoke, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)
        if fire_count > 0:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            alert_text_time = f"Time: {now}"
            alert_text_situation = f"Situation: Fire({fire_count}), Smoke({smoke_count})"
            cv2.rectangle(annotated_frame, (annotated_frame.shape[1] - 380, 5), (annotated_frame.shape[1] - 5, 65), (0, 0, 255), -1)
            cv2.putText(annotated_frame, alert_text_time, (annotated_frame.shape[1] - 375, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(annotated_frame, alert_text_situation, (annotated_frame.shape[1] - 375, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        return av.VideoFrame.from_ndarray(annotated_frame, format="bgr24")

    webrtc_streamer(
        key="webcam",
        video_frame_callback=video_frame_callback,
        media_stream_constraints={"video": True, "audio": False},
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )
    st.info("ì›¹ìº ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. í™”ì¬ ê°ì§€ ì‹œ ìš°ì¸¡ ìƒë‹¨ì— ê²½ê³ ê°€ í‘œì‹œë©ë‹ˆë‹¤.")