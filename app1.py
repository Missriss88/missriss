# app1.py
import os
import math
from typing import Tuple, List

import cv2
import av
import numpy as np
import streamlit as st
# --- Streamlit<->webrtc í˜¸í™˜ ì…”ì„(ì¼ë¶€ webrtc ë²„ì „ì´ experimental_rerun í˜¸ì¶œ) ---
if not hasattr(st, "experimental_rerun") and hasattr(st, "rerun"):
    st.experimental_rerun = st.rerun

from streamlit_webrtc import webrtc_streamer, WebRtcMode

import fire_detector
import person_detector

st.set_page_config(page_title="ì‹¤ì‹œê°„ í™”ì¬/ì¸ëª… ê°ì§€", page_icon="ğŸ”¥", layout="wide")

# =========================
# ê³ ì • ëª¨ë¸ ê²½ë¡œ(í•„ìš” ì‹œ ìˆ˜ì •)
# =========================
FIRE_PT   = os.path.abspath("fire2.pt")      # ì˜ˆ: ./fire2.pt
PERSON_PT = os.path.abspath("yolov8n.pt")    # ì˜ˆ: ./yolov8n.pt  (YOLOv5ë©´ yolov5s.pt)
PROXIMITY_PX_DEFAULT = 150                   # í™”ì¬-ì‚¬ëŒ ì¤‘ì‹¬ê±°ë¦¬ ê²½ê³  ì„ê³„ê°’(px)

# =========================
# ìœ í‹¸
# =========================
def center_distance(a, b) -> float:
    ax = (a[0]+a[2])/2.0; ay = (a[1]+a[3])/2.0
    bx = (b[0]+b[2])/2.0; by = (b[1]+b[3])/2.0
    return math.hypot(ax-bx, ay-by)

def draw_boxes(frame_bgr, fire_boxes, person_boxes, warning: bool):
    out = frame_bgr.copy()
    for (x1,y1,x2,y2) in fire_boxes:
        cv2.rectangle(out,(x1,y1),(x2,y2),(0,0,255),2)
        cv2.putText(out,"FIRE",(x1,max(0,y1-6)),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,0,255),2)
    for (x1,y1,x2,y2) in person_boxes:
        cv2.rectangle(out,(x1,y1),(x2,y2),(0,200,0),2)
        cv2.putText(out,"PERSON",(x1,max(0,y1-6)),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,200,0),2)
    if warning:
        h,w = out.shape[:2]
        cv2.rectangle(out,(0,0),(w,36),(0,0,255),-1)
        cv2.putText(out,"WARNING: PERSON NEAR FIRE",(10,26),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)
    return out

def rule_based_report(fire_count:int, person_count:int, warning:bool)->str:
    if fire_count==0 and person_count==0:
        risk="ì•ˆì „"; summary="í™”ì¬ì™€ ì‚¬ëŒ ëª¨ë‘ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."; action="ë³„ë„ ì¡°ì¹˜ í•„ìš” ì—†ìŠµë‹ˆë‹¤."
    elif fire_count>0 and person_count==0:
        risk="ì£¼ì˜"; summary="ì‘ì€ ë¶ˆì´ íƒì§€ë˜ì—ˆì§€ë§Œ ì£¼ë³€ì— ì‚¬ëŒì€ ì—†ìŠµë‹ˆë‹¤."; action="ìƒí™©ì„ ê´€ì°°í•˜ì„¸ìš”. í™•ì‚°/ì—°ê¸° ì¦ê°€ ì‹œ ì§„í™” ì¡°ì¹˜ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
    elif fire_count>0 and person_count>0 and not warning:
        risk="ì£¼ì˜"; summary="ë¶ˆì´ ìˆìœ¼ë‚˜ ì‚¬ëŒì€ ì•ˆì „ê±°ë¦¬ì—ì„œ ê´€ì°° ì¤‘ì…ë‹ˆë‹¤."; action="ì•ˆì „ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ê³  ì†Œí™”ê¸° ë“± ëŒ€ë¹„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."
    else:
        risk="ê²½ê³ "; summary="ë¶ˆ ê·¼ì²˜ì—ì„œ ì‚¬ëŒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."; action="ì¦‰ì‹œ ì•ˆì „ê±°ë¦¬ë¥¼ í™•ë³´í•˜ê³  í•„ìš” ì‹œ ì†Œí™”/ëŒ€í”¼ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”."
    return f"""[ìƒí™© ìš”ì•½]
- í™”ì¬: {fire_count}ê±´ / ì¸ì›: {person_count}ëª… / ê·¼ì ‘ ê²½ê³ : {'ë°œìƒ' if warning else 'ì—†ìŒ'}

[ìœ„í—˜ í‰ê°€] {risk}
{summary}

[ê¶Œì¥ ì¡°ì¹˜]
- {action}
"""

@st.cache_resource
def load_models(fire_pt_path: str, person_pt_path: str):
    fire_model = fire_detector.load_fire_model(fire_pt_path)
    person_model = person_detector.load_person_model(person_pt_path)
    return fire_model, person_model

def detect_once(frame_bgr: np.ndarray, fire_model, person_model, proximity_px:int):
    fire_boxes  = fire_detector.detect_fire(frame_bgr, fire_model) or []
    person_boxes= person_detector.detect_person(frame_bgr, person_model) or []
    warning=False
    for fb in fire_boxes:
        for pb in person_boxes:
            if center_distance(fb,pb) <= proximity_px:
                warning=True; break
        if warning: break
    annotated = draw_boxes(frame_bgr, fire_boxes, person_boxes, warning)
    return annotated, len(fire_boxes), len(person_boxes), warning

# =========================
# UI
# =========================
st.title("ğŸ”¥ ì‹¤ì‹œê°„ í™”ì¬/ì¸ëª… ê°ì§€")

# ëª¨ë¸ ë¡œë”©
try:
    with st.spinner("AI ëª¨ë¸ ë¡œë”© ì¤‘..."):
        fire_model, person_model = load_models(FIRE_PT, PERSON_PT)
    st.success("ëª¨ë¸ ë¡œë”© ì™„ë£Œ âœ…")
except Exception as e:
    st.error("AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ âŒ .pt íŒŒì¼ê³¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.exception(e)
    st.stop()

mode = st.radio("ëª¨ë“œ ì„ íƒ", ["ì›¹ìº ", "ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ë™ì˜ìƒ ì—…ë¡œë“œ"], horizontal=True)

# ìƒíƒœ ë³´ë“œ(ì›¹ìº ì—ì„œ ì‚¬ìš©)
status_fire = st.empty(); status_person = st.empty(); status_warn = st.empty()

# ======================================
# 1) ì›¹ìº 
# ======================================
if mode == "ì›¹ìº ":
    st.subheader("ì›¹ìº  ì‹¤ì‹œê°„ ê°ì§€")
    proximity_px = st.slider("ìœ„í—˜ ê·¼ì ‘ ê±°ë¦¬(px)", 50, 500, PROXIMITY_PX_DEFAULT)
    if "report_text" not in st.session_state:
        st.session_state.report_text = ""
    if "report_done" not in st.session_state:
        st.session_state.report_done = False

    def video_frame_callback(frame: av.VideoFrame) -> av.VideoFrame:
        frm = frame.to_ndarray(format="bgr24")
        frm = cv2.flip(frm, 1)
        annotated, f_cnt, p_cnt, warn = detect_once(frm, fire_model, person_model, proximity_px)

        # ìƒíƒœ ë³´ë“œ ê°±ì‹ 
        status_fire.metric("íƒì§€ëœ í™”ì¬", f_cnt)
        status_person.metric("íƒì§€ëœ ì¸ì›", p_cnt)
        status_warn.metric("ê·¼ì ‘ ê²½ê³ ", "ë°œìƒ" if warn else "ì—†ìŒ")

        # ê²½ê³  ìµœì´ˆ ë°œìƒ ì‹œ 1íšŒ ë¦¬í¬íŠ¸ ìƒì„±
        if warn and not st.session_state.report_done:
            st.session_state.report_text = rule_based_report(f_cnt, p_cnt, warn)
            st.session_state.report_done = True

        return av.VideoFrame.from_ndarray(annotated, format="bgr24")

    ctx = webrtc_streamer(
        key="webcam",
        mode=WebRtcMode.SENDRECV,
        video_frame_callback=video_frame_callback,
        media_stream_constraints={"video": True, "audio": False},
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
        async_processing=True,
    )

    # ì¹´ë©”ë¼ ê¶Œí•œ/ì‹œì‘ ì•ˆë‚´
    if not ctx or not ctx.state.playing:
        st.info("ì¢Œì¸¡ í•˜ë‹¨ì˜ **START**ë¥¼ ëˆ„ë¥´ê³  ë¸Œë¼ìš°ì €ì˜ **ì¹´ë©”ë¼ ê¶Œí•œ í—ˆìš©**ì„ í•´ì£¼ì„¸ìš”.")

    # ë¦¬í¬íŠ¸ í‘œì‹œ/ì´ˆê¸°í™”
    if st.session_state.report_done and st.session_state.report_text:
        st.warning("ğŸš¨ ìœ„í—˜ ìƒí™© ê°ì§€ë¨. ì•„ë˜ ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.text_area("í˜„ì¥ ë¦¬í¬íŠ¸(ê°„ê²°, ê³¼ì¥ ê¸ˆì§€)", st.session_state.report_text, height=240)
        c1, c2 = st.columns(2)
        if c1.button("ë¦¬í¬íŠ¸ ì´ˆê¸°í™”"):
            st.session_state.report_text = ""; st.session_state.report_done = False; st.rerun()
        if c2.button("ê·¼ì ‘ ì„ê³„ê°’ +20px"):
            proximity_px = min(500, proximity_px + 20)
            st.toast(f"ì„ê³„ê°’ì„ {proximity_px-20} â†’ {proximity_px}pxë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.")

# ======================================
# 2) ì´ë¯¸ì§€ ì—…ë¡œë“œ
# ======================================
elif mode == "ì´ë¯¸ì§€ ì—…ë¡œë“œ":
    st.subheader("ì´ë¯¸ì§€ ì—…ë¡œë“œ ë¶„ì„")
    file = st.file_uploader("ì´ë¯¸ì§€ ì„ íƒ(jpg/png)", type=["jpg","jpeg","png"])
    proximity_px = st.slider("ìœ„í—˜ ê·¼ì ‘ ê±°ë¦¬(px)", 50, 500, PROXIMITY_PX_DEFAULT, key="imgprox")
    if file:
        file_bytes = np.frombuffer(file.read(), np.uint8)
        img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
        annotated, f_cnt, p_cnt, warn = detect_once(img, fire_model, person_model, proximity_px)
        st.image(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB), caption="ë¶„ì„ ê²°ê³¼", use_container_width=True)
        st.markdown(rule_based_report(f_cnt, p_cnt, warn))

# ======================================
# 3) ë™ì˜ìƒ ì—…ë¡œë“œ
# ======================================
else:
    st.subheader("ë™ì˜ìƒ ì—…ë¡œë“œ ë¶„ì„")
    file = st.file_uploader("ë™ì˜ìƒ ì„ íƒ(mp4/avi/mov)", type=["mp4","avi","mov"])
    proximity_px = st.slider("ìœ„í—˜ ê·¼ì ‘ ê±°ë¦¬(px)", 50, 500, PROXIMITY_PX_DEFAULT, key="vidprox")
    if file:
        # ì„ì‹œ ì €ì¥ í›„ OpenCVë¡œ í”„ë ˆì„ ì²˜ë¦¬
        tmp_path = os.path.abspath("uploaded_video.tmp")
        with open(tmp_path, "wb") as f:
            f.write(file.read())
        cap = cv2.VideoCapture(tmp_path)
        viewer = st.empty()
        info = st.empty()
        frame_count = 0
        fire_total = person_total = warn_flag_total = 0

        while cap.isOpened():
            ok, frame = cap.read()
            if not ok: break
            frame_count += 1
            annotated, f_cnt, p_cnt, warn = detect_once(frame, fire_model, person_model, proximity_px)
            fire_total += f_cnt; person_total += p_cnt
            warn_flag_total = warn_flag_total or warn
            # í‘œì‹œ
            viewer.image(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB), use_container_width=True)
            info.text(f"í”„ë ˆì„: {frame_count}  |  í™”ì¬: {f_cnt}  ì¸ì›: {p_cnt}  ê²½ê³ : {'Y' if warn else 'N'}")
            # ì†ë„ ê³¼ë„ ë°©ì§€
            cv2.waitKey(1)

        cap.release()
        os.remove(tmp_path)
        st.success("ë¶„ì„ ì™„ë£Œ")
        st.markdown(rule_based_report(fire_total, person_total, warn_flag_total))
