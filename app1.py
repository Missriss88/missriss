# app.py
import streamlit as st
import cv2
import numpy as np
import toml
import google.generativeai as genai
from PIL import Image
import tempfile
import time
import math
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase

# ì œê³µëœ íƒì§€ ëª¨ë“ˆ ì„í¬íŠ¸
from fire_detector import load_fire_model, detect_fire
from person_detector import load_person_model, detect_person

# --- ì´ˆê¸° ì„¤ì • ë° ëª¨ë¸ ë¡œë”© ---

st.set_page_config(page_title="ğŸ”¥ AI í™”ì¬ ë° ì¸ëª… ì•ˆì „ ê´€ì œ ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ”¥ AI í™”ì¬ ë° ì¸ëª… ì•ˆì „ ê´€ì œ ì‹œìŠ¤í…œ")
st.write("ì´ë¯¸ì§€, ë™ì˜ìƒ ë˜ëŠ” ì›¹ìº ì„ í†µí•´ í™”ì¬ì™€ ì‚¬ëŒì„ ê°ì§€í•˜ê³  ìœ„í—˜ ìƒí™© ì‹œ AI ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

# @st.cache_resource: Streamlit ì•±ì˜ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ëª¨ë¸ì„ ìºì‹œì— ì €ì¥
@st.cache_resource
def load_models():
    """YOLO ëª¨ë¸ë“¤ì„ ë¡œë“œí•˜ê³  ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        fire_model = load_fire_model('fire2.pt')
        person_model = load_person_model('yolov8n.pt')
        return fire_model, person_model
    except FileNotFoundError as e:
        st.error(f"ëª¨ë¸ íŒŒì¼ ë¡œë”© ì˜¤ë¥˜: {e}. 'fire2.pt'ì™€ 'yolov8n.pt' íŒŒì¼ì´ í”„ë¡œì íŠ¸ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None, None

fire_model, person_model = load_models()

# Google API í‚¤ ë¡œë“œ ë° Gemini ì„¤ì •
try:
    secrets = toml.load("secret.toml")
    GOOGLE_API_KEY = secrets.get("GOOGLE_API_KEY")
    if not GOOGLE_API_KEY:
        st.error("secret.toml íŒŒì¼ì—ì„œ Google API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    genai.configure(api_key=GOOGLE_API_KEY)
except FileNotFoundError:
    st.error("'secret.toml' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---

def is_near(person_box, fire_box, threshold):
    """ì‚¬ëŒê³¼ ë¶ˆì˜ ë°”ìš´ë”© ë°•ìŠ¤ ì¤‘ì‹¬ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ì—¬ ê·¼ì ‘ ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤."""
    px, py = (person_box[0] + person_box[2]) / 2, (person_box[1] + person_box[3]) / 2
    fx, fy = (fire_box[0] + fire_box[2]) / 2, (fire_box[1] + fire_box[3]) / 2
    distance = math.sqrt((px - fx)**2 + (py - fy)**2)
    return distance < threshold

def process_frame(frame, proximity_threshold):
    """ë‹¨ì¼ í”„ë ˆì„ì— ëŒ€í•´ í™”ì¬ ë° ì‚¬ëŒ ê°ì§€, ìœ„í—˜ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    fire_boxes = detect_fire(frame, fire_model)
    person_boxes = detect_person(frame, person_model)
    
    dangerous_persons = set()

    # ìœ„í—˜ ìƒí™© ë¶„ì„
    for i, p_box in enumerate(person_boxes):
        for f_box in fire_boxes:
            if is_near(p_box, f_box, proximity_threshold):
                dangerous_persons.add(i)
                break

    # ì‹œê°í™”: ë°”ìš´ë”© ë°•ìŠ¤ ë° ê²½ê³  ê·¸ë¦¬ê¸°
    for i, p_box in enumerate(person_boxes):
        x1, y1, x2, y2 = p_box
        if i in dangerous_persons:
            # ìœ„í—˜ì— ì²˜í•œ ì‚¬ëŒ: ë¹¨ê°„ìƒ‰
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
            cv2.putText(frame, "DANGER", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)
        else:
            # ì•ˆì „í•œ ì‚¬ëŒ: ë…¹ìƒ‰
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, "Person", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    for f_box in fire_boxes:
        x1, y1, x2, y2 = f_box
        # í™”ì¬: ì£¼í™©ìƒ‰
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 165, 255), 2)
        cv2.putText(frame, "FIRE", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 165, 255), 2)

    # í™”ë©´ ìƒë‹¨ì— ê²½ê³  ë©”ì‹œì§€ í‘œì‹œ
    if dangerous_persons:
        warning_text = f"WARNING: {len(dangerous_persons)} person(s) near fire!"
        cv2.putText(frame, warning_text, (50, 50), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0, 0, 255), 3)
        
    return frame, len(fire_boxes), len(person_boxes), len(dangerous_persons)


def generate_ai_report(image, fire_count, person_count, danger_count):
    """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ AI ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    st.info("AI ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    
    model = genai.GenerativeModel('gemini-pro-vision')
    
    prompt = f"""
    ë‹¹ì‹ ì€ ìµœì²¨ë‹¨ ì¬ë‚œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì•„ë˜ ì´ë¯¸ì§€ëŠ” ì‹¤ì œ ì¬ë‚œ ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•œ ê²ƒì…ë‹ˆë‹¤.
    
    ë¶„ì„ ë°ì´í„°:
    - íƒì§€ëœ í™”ì¬ ìˆ˜: {fire_count}
    - íƒì§€ëœ ì‚¬ëŒ ìˆ˜: {person_count}
    - í™”ì¬ ê·¼ì²˜ ìœ„í—˜ ì¸ì› ìˆ˜: {danger_count}
    
    ìœ„ ë°ì´í„°ì™€ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì „ë¬¸ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
    1.  **ìƒí™© ê°œìš”**: í˜„ì¬ ì´ë¯¸ì§€ì— ë‚˜íƒ€ë‚œ ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
    2.  **ìœ„í—˜ í‰ê°€**: í™”ì¬ì˜ ê·œëª¨, ìœ„í—˜ì— ì²˜í•œ ì‚¬ëŒì˜ ìˆ˜ ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ í˜„ì¬ ìƒí™©ì˜ ì‹¬ê°ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.
    3.  **ê¶Œì¥ ì¡°ì¹˜**: ì´ ìƒí™©ì—ì„œ ì¦‰ì‹œ ì·¨í•´ì•¼ í•  í–‰ë™(ì˜ˆ: ëŒ€í”¼ ê²½ë¡œ ì•ˆë‚´, ì†Œë°©ì„œ ì‹ ê³ , íŠ¹ì • ì¸ë¬¼ ìš°ì„  êµ¬ì¡° ë“±)ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆí•©ë‹ˆë‹¤.
    """
    
    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    
    try:
        response = model.generate_content([prompt, pil_image])
        st.success("AI ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
        return response.text
    except Exception as e:
        st.error(f"AI ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

# --- Streamlit UI êµ¬ì„± ---

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("âš™ï¸ ì„¤ì •")
proximity_threshold = st.sidebar.slider("ìœ„í—˜ ê°ì§€ ì„ê³„ê°’ (ê±°ë¦¬)", 50, 500, 150, 10,
                                        help="í™”ì¬ì™€ ì‚¬ëŒ ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ì´ ê°’ë³´ë‹¤ ê°€ê¹Œìš°ë©´ 'ìœ„í—˜'ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤ (í”½ì…€ ë‹¨ìœ„).")

input_method = st.sidebar.radio("ì…ë ¥ ë°©ì‹ ì„ íƒ", ('ì´ë¯¸ì§€ ì—…ë¡œë“œ', 'ë™ì˜ìƒ ì—…ë¡œë“œ', 'ì‹¤ì‹œê°„ ì›¹ìº '))

# AI ë¦¬í¬íŠ¸ ì €ì¥ì„ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'ai_report' not in st.session_state:
    st.session_state.ai_report = ""

if fire_model is None or person_model is None:
    st.warning("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì•±ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    if input_method == 'ì´ë¯¸ì§€ ì—…ë¡œë“œ':
        uploaded_file = st.file_uploader("ì´ë¯¸ì§€ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”...", type=["jpg", "jpeg", "png"])
        if uploaded_file is not None:
            file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
            frame = cv2.imdecode(file_bytes, 1)

            st.image(frame, channels="BGR", caption="ì›ë³¸ ì´ë¯¸ì§€")
            
            with st.spinner('ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...'):
                processed_frame, fire_count, person_count, danger_count = process_frame(frame.copy(), proximity_threshold)
            
            st.image(processed_frame, channels="BGR", caption="ë¶„ì„ ê²°ê³¼")
            
            if st.button("AI ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"):
                report = generate_ai_report(processed_frame, fire_count, person_count, danger_count)
                st.session_state.ai_report = report


    elif input_method == 'ë™ì˜ìƒ ì—…ë¡œë“œ':
        uploaded_file = st.file_uploader("ë™ì˜ìƒ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”...", type=["mp4", "mov", "avi"])
        if uploaded_file is not None:
            tfile = tempfile.NamedTemporaryFile(delete=False)
            tfile.write(uploaded_file.read())
            
            cap = cv2.VideoCapture(tfile.name)
            st_frame = st.empty()
            
            # AI ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ í”„ë ˆì„ ìº¡ì³
            report_capture_frame = None

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                processed_frame, fire_count, person_count, danger_count = process_frame(frame.copy(), proximity_threshold)
                st_frame.image(processed_frame, channels="BGR")
                
                # ìœ„í—˜ ìƒí™© ë°œìƒ ì‹œ ì²« í”„ë ˆì„ì„ ë¦¬í¬íŠ¸ìš©ìœ¼ë¡œ ìº¡ì³
                if danger_count > 0 and report_capture_frame is None:
                    report_capture_frame = processed_frame.copy()

            cap.release()
            
            if report_capture_frame is not None:
                st.warning("ë™ì˜ìƒì—ì„œ ìœ„í—˜ ìƒí™©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                if st.button("ìº¡ì³ëœ ìœ„í—˜ ìƒí™©ìœ¼ë¡œ AI ë¦¬í¬íŠ¸ ìƒì„±"):
                     report = generate_ai_report(report_capture_frame, fire_count, person_count, danger_count)
                     st.session_state.ai_report = report
            else:
                st.info("ë™ì˜ìƒ ë¶„ì„ ì™„ë£Œ. ê°ì§€ëœ ìœ„í—˜ ìƒí™©ì´ ì—†ìŠµë‹ˆë‹¤.")

    elif input_method == 'ì‹¤ì‹œê°„ ì›¹ìº ':
        st.info("ì›¹ìº ì„ ì‹œì‘í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì €ì—ì„œ ì¹´ë©”ë¼ ì ‘ê·¼ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.")

        class VideoTransformer(VideoTransformerBase):
            def __init__(self):
                self.proximity_threshold = 150 # ì´ˆê¸°ê°’

            def recv(self, frame):
                img = frame.to_ndarray(format="bgr24")
                
                # ì‚¬ì´ë“œë°” ê°’ ì‹¤ì‹œê°„ ë°˜ì˜ (ì´ ë¶€ë¶„ì´ ì˜ë„ëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠìŒ)
                self.proximity_threshold = proximity_threshold 
                
                processed_img, _, _, danger_count = process_frame(img, self.proximity_threshold)
                
                # ì‹¤ì‹œê°„ ìœ„í—˜ ê²½ê³ 
                if danger_count > 0:
                    cv2.putText(processed_img, "!! REAL-TIME DANGER !!", (50, 100), cv2.FONT_HERSHEY_TRIPLEX, 1.2, (0, 0, 255), 2)
                
                return processed_img
        
        webrtc_streamer(key="webcam", video_processor_factory=VideoTransformer)

    # AI ë¦¬í¬íŠ¸ ì¶œë ¥ ì˜ì—­
    if st.session_state.ai_report:
        st.markdown("---")
        st.subheader("ğŸ¤– AI ë¶„ì„ ë¦¬í¬íŠ¸")
        st.markdown(st.session_state.ai_report)
        if st.button("ë¦¬í¬íŠ¸ ì´ˆê¸°í™”"):
            st.session_state.ai_report = ""
            st.rerun()
